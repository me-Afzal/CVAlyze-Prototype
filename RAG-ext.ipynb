{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4b20b26-cf69-4a2d-a964-98b286b40cc8",
   "metadata": {},
   "source": [
    "# Extraction Strategy for Unstructured (CV/Resume) -> Structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58122174-e1e5-4529-b764-684eb60f519c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import Libraries\n",
    "import fitz\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba10b94-80ad-4501-92c0-ddc185f3e4b0",
   "metadata": {},
   "source": [
    "#### Extract text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2bbe422-9d35-410a-b2e5-2aa85aeaab03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'--- Page 1 ---\\nAFZAL A Data Scientist +91 7356922047 # afzalkottukkal23@gmail.com LinkedIn ¬ß GitHub Professional Summary Data Scientist and AI/ML Engineer with hands-on experience in statistical analysis, machine learning, deep learning, and big data engineering. Skilled in developing end-to-end data-driven solutions using tools and frameworks such as Python, TensorFlow, scikit-learn, FastAPI, and Apache Spark. Proficient in NLP, Computer Vision, and Transformer-based Large Language Models (LLMs) for advanced AI applications, including Generative AI (VAE, GAN). Experienced with SQL, BI tools, and cloud computing platforms for scalable data solutions. Strong understanding of microservices architecture, Hadoop, and Kafka for distributed processing and real-time analytics. Holds a Bachelor‚Äôs degree in Commerce (Accounts and Data Science, 2021‚Äì2024) and specialized in Data Science through Brototype, combining technical depth with domain knowledge to deliver innovative, business-driven outcomes. Technical Skills Programming & Libraries: Python, TensorFlow, PyTorch, scikit-learn, Pandas, NumPy, OpenCV, Transformers (LLMs), NLP, Computer Vision, Generative AI (VAE, GAN) Big Data & Cloud: Apache Spark, Hadoop, Kafka, AWS, GCP, Azure Database & Querying: SQL, PostgreSQL, MySQL, Query Optimization, Database Design Web & Deployment: FastAPI, Microservices, Docker, Kubernetes, REST APIs, JWT Authentication Visualization & BI Tools: Matplotlib, Seaborn, Power BI, Tableau, Data Storytelling Software Practices: System Design, Data Structures and Algorithms Professional Projects CVAlyze ‚Äî AI-Powered ETL + CV Analysis Platform (Ongoing Development) An intelligent ETL pipeline currently in development that transforms unstructured CV data into structured formats using a hybrid Regex + RAG (Retrieval-Augmented Generation) approach. ‚Äì Designed an AI-driven resume parsing system integrating Gemini API and LangChain RAG. ‚Äì Implemented modular data extraction and preprocessing pipeline for name, contact, skills, and experience. ‚Äì Optimized performance for large-scale document processing through asynchronous execution. ‚Äì Project is ongoing and actively under development to enhance accuracy and scalability. Image Enhancer Pro ‚Äî ESRGAN Image Super-Resolution API GitHub A high-quality image enhancement API providing 4x super-resolution using Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN). ‚Äì Built using FastAPI and PyTorch for scalable inference and real-time enhancement. ‚Äì Integrated ESRGAN pretrained weights for fine-grained detail recovery and denoising. ‚Äì Implemented optimized I/O pipeline for batch image processing. Bank Analytics Dashboard GitHub A comprehensive analytics dashboard offering insights into customer behavior, deposits, and loan performance. ‚Äì Developed interactive visualizations using Power BI and Python-based analytics. ‚Äì Applied machine learning models for customer segmentation and risk prediction. ‚Äì Enabled real-time KPI tracking and automated report generation. Article Research Tool (RAG App) Live Demo GitHub A Retrieval-Augmented Generation (RAG) application for intelligent article summarization and information extraction. ‚Äì Integrated LangChain with vector-based retrieval for contextual document search. ‚Äì Built semantic search using OpenAI embeddings and FAISS indexing. ‚Äì Deployed on Streamlit with modern UI for research-oriented workflows. True Buddy Chatbot ‚Äî Emotional Support AI Live Demo GitHub An intelligent conversational AI providing emotional support using transformer-based models. ‚Äì Implemented contextual memory for continuous and empathetic dialogue generation. ‚Äì Leveraged LLMs via API for emotional tone detection and adaptive responses.\\n\\nLinks: https://article-researcher-app-cbngx5mmmxxdtjzfewngtn.streamlit.app/, https://github.com/me-Afzal/, https://github.com/me-Afzal/Article-Researcher-app, https://github.com/me-Afzal/Bank_Analysis, https://github.com/me-Afzal/Data-ETL-Pipeline, https://github.com/me-Afzal/Fake-News-Detection, https://github.com/me-Afzal/Harry_Potter_Cloak_Invisibility, https://github.com/me-Afzal/Hybrid-movie-recommendation-app, https://github.com/me-Afzal/IPL_win_probability_predictor, https://github.com/me-Afzal/Image-Enhancer-Pro, https://github.com/me-Afzal/Medical-cost-predictor, https://github.com/me-Afzal/NoteKeeper, https://github.com/me-Afzal/True-Buddy-Chatbot, https://hybrid-movie-recommend-app.streamlit.app/, https://ipl-win-probability-predictor-tool.streamlit.app/, https://linkedin.com/in/afzal-a-0b1962325, https://medical-cost-predictor-web.streamlit.app/, https://true-buddy-chatbot.streamlit.app/\\n\\n--- Page 2 ---\\n‚Äì Deployed as a Streamlit web app with minimal UI for mental health engagement. Medical Insurance Cost Predictor Live Demo GitHub A regression-based ML model predicting medical insurance costs with automated report generation. ‚Äì Implemented data preprocessing and regression modeling using scikit-learn. ‚Äì Integrated dynamic report creation in PDF format for user predictions. ‚Äì Deployed end-to-end ML pipeline with interactive web interface via Streamlit. NoteKeeper ‚Äî FastAPI Microservices Application GitHub A microservices-based note-taking platform with JWT-based authentication and modular architecture. ‚Äì Developed a FastAPI gateway managing authentication and service routing. ‚Äì Implemented user and note microservices communicating through internal APIs. ‚Äì Integrated JWT middleware ensuring secure and stateless authentication. IPL Win Probability Predictor Live Demo GitHub An AI-powered application predicting live IPL match win probabilities using real-time ML inference. ‚Äì Trained machine learning models using past IPL datasets and live match updates. ‚Äì Developed interactive dashboard with glassmorphism design using Streamlit. ‚Äì Optimized prediction latency for real-time user interaction. Additional Projects Movie Recommender Pro (Hybrid) Live Demo GitHub An advanced recommendation system combining collaborative filtering and content-based filtering for personalized movie suggestions. ‚Äì Implemented hybrid recommendation using cosine similarity and user-item interaction matrices. ‚Äì Built with Streamlit for interactive user experience and real-time recommendations. ‚Äì Integrated scalable model serving pipeline for fast query response. Fake News Detector GitHub An AI-powered web application that detects fake news using Natural Language Processing (NLP) and Machine Learning. ‚Äì Trained and evaluated ML models using TF-IDF, Logistic Regression, and Random Forest. ‚Äì Built explainable prediction pipeline for transparency in classification. ‚Äì Deployed interactive inference interface using Streamlit. Harry Potter Invisibility Cloak ‚Äî Computer Vision App GitHub A real-time computer vision project that creates an invisibility effect using OpenCV and HSV color masking. ‚Äì Implemented background subtraction and color segmentation for dynamic masking. ‚Äì Integrated face detection using LBP Cascade Classifier for enhanced user interaction. ‚Äì Created AR-style invisibility illusion with optimized frame processing. Data ETL Pipeline ‚Äî Warehouse Analytics Platform GitHub A comprehensive data pipeline for automating warehouse shipment analytics using Google Cloud Platform (GCP) services. ‚Äì Designed an ETL pipeline for CSV ingestion, cleaning, and transformation. ‚Äì Integrated with BigQuery and Cloud Storage for real-time analytics. ‚Äì Enabled business intelligence insights through automated data visualization dashboards. Education & Training Brototype ‚Äî Data Science and AI/ML Program Aug. 2024 ‚Äì Present Remote / Kochi, Kerala Comprehensive 1-year Data Science specialization program focused on Machine Learning, Deep Learning, MLOps, and Cloud Deployment. St. John‚Äôs College, Anchal (University of Kerala) Aug. 2021 ‚Äì Apr. 2024 Kollam, Kerala Bachelor of Commerce in Accounts and Data Science. CPHSS Kadakkal Jun. 2019 ‚Äì May. 2021 Kollam, Kerala Higher Secondary Education in Commerce with Computer Applications.\\n\\n--- Page 3 ---\\nKey Achievements & Certifications ‚Ä¢ Developed and deployed multiple AI-powered applications leveraging FastAPI, LangChain, and LLMs for intelligent automation and RAG-based systems. ‚Ä¢ Implemented end-to-end ETL pipelines using Apache Airflow on AWS and GCP Cloud Run, BigQuery, Cloud SQL, and Cloud Storage. ‚Ä¢ Built and fine-tuned machine learning and deep learning models for real-world use cases in prediction, computer vision, and generative AI. ‚Ä¢ Deployed scalable microservice architectures on Google Cloud Kubernetes Engine (GKE) and achieved high reliability across services. ‚Ä¢ Integrated MLOps practices including CI/CD pipelines for model deployment and monitoring in production. ‚Ä¢ Hands-on expertise in Data Analytics, Python, SQL, and BI tools for actionable insights and data storytelling. ‚Ä¢ Earned certifications in Data Science, Machine Learning, and Cloud Computing through Brototype and self-driven projects.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Hybrid extractor for CVs.\n",
    "    Captures both clickable and plain-text URLs (including Streamlit, HuggingFace, etc.)\n",
    "    Inserts only meaningful top-level links (GitHub, LinkedIn, portfolio/demo).\n",
    "    \"\"\"\n",
    "    final_text = \"\"\n",
    "    all_links = set()\n",
    "\n",
    "    # Extract embedded hyperlinks\n",
    "    with fitz.open(pdf_path) as doc:\n",
    "        for page in doc:\n",
    "            for link in page.get_links():\n",
    "                uri = link.get(\"uri\")\n",
    "                if uri and uri.startswith(\"http\"):\n",
    "                    all_links.add(uri.strip())\n",
    "\n",
    "    # Detect plain-text URLs\n",
    "    url_pattern = re.compile(\n",
    "        r'(https?://[^\\s]+|www\\.[^\\s]+|\\b[\\w-]+\\.(?:vercel|netlify|github|streamlit|huggingface|render|heroku|io|app|ai|com|org)\\b[^\\s]*)',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i, page in enumerate(pdf.pages, start=1):\n",
    "            text = page.extract_text() or \"\"\n",
    "\n",
    "            # Extract any URLs written in text\n",
    "            found_urls = url_pattern.findall(text)\n",
    "            for u in found_urls:\n",
    "                clean_url = u.strip(\").,;:!?\")\n",
    "                all_links.add(clean_url)\n",
    "\n",
    "            # Clean formatting junk\n",
    "            text = re.sub(r\"\\(cid:\\d+\\)\", \"\", text)\n",
    "            text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "            # Insert top-level links (only once)\n",
    "            if i == 1:\n",
    "                top_links = []\n",
    "                for link in all_links:\n",
    "                    if any(\n",
    "                        k in link.lower()\n",
    "                        for k in [\n",
    "                            \"linkedin\",\n",
    "                            \"github\",\n",
    "                            \"portfolio\",\n",
    "                            \"vercel\",\n",
    "                            \"netlify\",\n",
    "                            \"streamlit\",\n",
    "                            \"huggingface\",\n",
    "                            \"render\",\n",
    "                            \"demo\",\n",
    "                            \"live\",\n",
    "                            \"project\",\n",
    "                        ]\n",
    "                    ):\n",
    "                        top_links.append(link)\n",
    "\n",
    "                if top_links:\n",
    "                    text = text.strip() + \"\\n\\nLinks: \" + \", \".join(sorted(top_links))\n",
    "\n",
    "            final_text += f\"\\n\\n--- Page {i} ---\\n{text.strip()}\"\n",
    "\n",
    "    return final_text.strip()\n",
    "\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"og_cv/afzal.pdf\")\n",
    "pdf_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7cd32b-c783-4277-97c1-0f7f06aa0109",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Extract text from DOCX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f58357c-391b-4f4d-8b6c-192c0c135a10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curriculum Vitae\n",
      "Afzal A\n",
      "üìç Bangalore, India | üìß afzal@email.com | üì± +91-9876543210\n",
      "üåê www.afzala.com | üíº LinkedIn: linkedin.com/in/afzala | üêô GitHub: github.com/afzala\n",
      "Profile Summary\n",
      "Data Scientist & Machine Learning Engineer with 3+ years of experience building end-to-end AI solutions. Specialized in retail analytics, demand forecasting, dynamic pricing, and inventory optimization. Strong background in Python, SQL, and cloud platforms (AWS, GCP). Passionate about applying AI to solve real-world business problems.\n",
      "Skills\n",
      "Programming & Tools: Python, R, SQL, Spark, TensorFlow, PyTorch, scikit-learn, Tableau, Power BI\n",
      "Specializations: Retail Analytics, Machine Learning, NLP, Time Series Forecasting, Recommender Systems\n",
      "Cloud & Deployment: AWS (SageMaker, Lambda), GCP (Vertex AI, BigQuery), Docker, Kubernetes, FastAPI\n",
      "Soft Skills: Problem Solving, Team Collaboration, Communication, Analytical Thinking\n",
      "Work Experience\n",
      "Data Scientist ‚Äì Walmart Labs, Bangalore\n",
      "Jan 2022 ‚Äì Present\n",
      "- Built demand forecasting models using LSTMs and XGBoost, improving forecast accuracy by 18%.\n",
      "- Developed dynamic pricing system that adjusted prices based on competitor data & demand, boosting revenue by 12%.\n",
      "- Automated inventory management dashboards using Tableau + Python, reducing stockouts by 20%.\n",
      "- Collaborated with product and business teams to deploy ML models into production on GCP.\n",
      "Machine Learning Engineer ‚Äì Target, Bangalore\n",
      "Jul 2020 ‚Äì Dec 2021\n",
      "- Designed market basket analysis models to recommend product bundles, increasing cross-sell revenue.\n",
      "- Implemented A/B testing automation with Python to optimize marketing campaigns.\n",
      "- Built customer segmentation models with K-Means and DBSCAN for personalized campaigns.\n",
      "Education\n",
      "M.Tech, Data Science ‚Äì Indian Institute of Technology (IIT), 2020\n",
      "B.E., Computer Science ‚Äì Visvesvaraya Technological University (VTU), 2018\n",
      "Projects\n",
      "- Retail Analytics Dashboard ‚Äì End-to-end solution for sales forecasting, pricing, and recommendations using Python + Tableau.\n",
      "- Fraud Detection System ‚Äì Credit card fraud detection using Random Forest and One-Class SVM.\n",
      "- E-commerce Chatbot ‚Äì Built an AI-powered chatbot using FastAPI + GPT for product recommendations.\n",
      "Certifications\n",
      "AWS Certified Machine Learning ‚Äì Specialty\n",
      "Google Cloud Professional Data Engineer\n",
      "Advanced SQL for Data Science ‚Äì Coursera\n",
      "Achievements\n",
      "- Won Top 10 AI Innovation Award at Walmart Hackathon 2023.\n",
      "- Published research paper on ‚ÄúAI-driven Pricing Optimization in Retail‚Äù at IEEE ICMLA 2022.\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "docx_text = extract_text_from_docx(\"cv/demo_cv.docx\")\n",
    "print(docx_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "494a348b-ac59-4649-9eb3-ce2fad992a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Page 1 ---\n",
      "AFZAL A Data Scientist +91 7356922047 # afzalkottukkal23@gmail.com LinkedIn ¬ß GitHub Professional Summary Data Scientist and AI/ML Engineer with hands-on experience in statistical analysis, machine learning, deep learning, and big data engineering. Skilled in developing end-to-end data-driven solutions using tools and frameworks such as Python, TensorFlow, scikit-learn, FastAPI, and Apache Spark. Proficient in NLP, Computer Vision, and Transformer-based Large Language Models (LLMs\n"
     ]
    }
   ],
   "source": [
    "# Extract function \n",
    "def extract_text(file_path):\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "    \n",
    "    if ext == \".pdf\":\n",
    "        return extract_text_from_pdf(file_path)\n",
    "    elif ext == \".docx\":\n",
    "        return extract_text_from_docx(file_path)\n",
    "    elif ext == \".txt\":\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format: \" + ext)\n",
    "# Example usage\n",
    "text = extract_text(\"og_cv/afzal.pdf\")\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84273ee2-fac2-43ef-b85b-2ed8c3eb1729",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFZAL A Data Scientist +91 7356922047 # afzalkottukkal23@gmail.com LinkedIn GitHub Professional Summary Data Scientist and AI/ML Engineer with hands-on experience in statistical analysis, machine learning, deep learning, and big data engineering. Skilled in developing end-to-end data-driven solutions using tools and frameworks such as Python, TensorFlow, scikit-learn, FastAPI, and Apache Spark. Proficient in NLP, Computer Vision, and Transformer-based Large Language Models (LLMs) for advanced AI applications, including Generative AI (VAE, GAN). Experienced with SQL, BI tools, and cloud computing platforms for scalable data solutions. Strong understanding of microservices architecture, Hadoop, and Kafka for distributed processing and real-time analytics. Holds a Bachelor's degree in Commerce (Accounts and Data Science, 2021-2024) and specialized in Data Science through Brototype, combining technical depth with domain knowledge to deliver innovative, business-driven outcomes. Technical Skills Programming & Libraries: Python, TensorFlow, PyTorch, scikit-learn, Pandas, NumPy, OpenCV, Transformers (LLMs), NLP, Computer Vision, Generative AI (VAE, GAN) Big Data & Cloud: Apache Spark, Hadoop, Kafka, AWS, GCP, Azure Database & Querying: SQL, PostgreSQL, MySQL, Query Optimization, Database Design Web & Deployment: FastAPI, Microservices, Docker, Kubernetes, REST APIs, JWT Authentication Visualization & BI Tools: Matplotlib, Seaborn, Power BI, Tableau, Data Storytelling Software Practices: System Design, Data Structures and Algorithms Professional Projects CVAlyze - AI-Powered ETL + CV Analysis Platform (Ongoing Development) An intelligent ETL pipeline currently in development that transforms unstructured CV data into structured formats using a hybrid Regex + RAG (Retrieval-Augmented Generation) approach. - Designed an AI-driven resume parsing system integrating Gemini API and LangChain RAG. - Implemented modular data extraction and preprocessing pipeline for name, contact, skills, and experience. - Optimized performance for large-scale document processing through asynchronous execution. - Project is ongoing and actively under development to enhance accuracy and scalability. Image Enhancer Pro - ESRGAN Image Super-Resolution API GitHub A high-quality image enhancement API providing 4x super-resolution using Enhanced Super-Resolution Generative Adversarial Networks (ESRGAN). - Built using FastAPI and PyTorch for scalable inference and real-time enhancement. - Integrated ESRGAN pretrained weights for fine-grained detail recovery and denoising. - Implemented optimized I/O pipeline for batch image processing. Bank Analytics Dashboard GitHub A comprehensive analytics dashboard offering insights into customer behavior, deposits, and loan performance. - Developed interactive visualizations using Power BI and Python-based analytics. - Applied machine learning models for customer segmentation and risk prediction. - Enabled real-time KPI tracking and automated report generation. Article Research Tool (RAG App) Live Demo GitHub A Retrieval-Augmented Generation (RAG) application for intelligent article summarization and information extraction. - Integrated LangChain with vector-based retrieval for contextual document search. - Built semantic search using OpenAI embeddings and FAISS indexing. - Deployed on Streamlit with modern UI for research-oriented workflows. True Buddy Chatbot - Emotional Support AI Live Demo GitHub An intelligent conversational AI providing emotional support using transformer-based models. - Implemented contextual memory for continuous and empathetic dialogue generation. - Leveraged LLMs via API for emotional tone detection and adaptive responses.\n",
      "Links: https://article-researcher-app-cbngx5mmmxxdtjzfewngtn.streamlit.app/, https://github.com/me-Afzal/, https://github.com/me-Afzal/Article-Researcher-app, https://github.com/me-Afzal/Bank_Analysis, https://github.com/me-Afzal/Data-ETL-Pipeline, https://github.com/me-Afzal/Fake-News-Detection, https://github.com/me-Afzal/Harry_Potter_Cloak_Invisibility, https://github.com/me-Afzal/Hybrid-movie-recommendation-app, https://github.com/me-Afzal/IPL_win_probability_predictor, https://github.com/me-Afzal/Image-Enhancer-Pro, https://github.com/me-Afzal/Medical-cost-predictor, https://github.com/me-Afzal/NoteKeeper, https://github.com/me-Afzal/True-Buddy-Chatbot, https://hybrid-movie-recommend-app.streamlit.app/, https://ipl-win-probability-predictor-tool.streamlit.app/, https://linkedin.com/in/afzal-a-0b1962325, https://medical-cost-predictor-web.streamlit.app/, https://true-buddy-chatbot.streamlit.app/ - Deployed as a Streamlit web app with minimal UI for mental health engagement. Medical Insurance Cost Predictor Live Demo GitHub A regression-based ML model predicting medical insurance costs with automated report generation. - Implemented data preprocessing and regression modeling using scikit-learn. - Integrated dynamic report creation in PDF format for user predictions. - Deployed end-to-end ML pipeline with interactive web interface via Streamlit. NoteKeeper - FastAPI Microservices Application GitHub A microservices-based note-taking platform with JWT-based authentication and modular architecture. - Developed a FastAPI gateway managing authentication and service routing. - Implemented user and note microservices communicating through internal APIs. - Integrated JWT middleware ensuring secure and stateless authentication. IPL Win Probability Predictor Live Demo GitHub An AI-powered application predicting live IPL match win probabilities using real-time ML inference. - Trained machine learning models using past IPL datasets and live match updates. - Developed interactive dashboard with glassmorphism design using Streamlit. - Optimized prediction latency for real-time user interaction. Additional Projects Movie Recommender Pro (Hybrid) Live Demo GitHub An advanced recommendation system combining collaborative filtering and content-based filtering for personalized movie suggestions. - Implemented hybrid recommendation using cosine similarity and user-item interaction matrices. - Built with Streamlit for interactive user experience and real-time recommendations. - Integrated scalable model serving pipeline for fast query response. Fake News Detector GitHub An AI-powered web application that detects fake news using Natural Language Processing (NLP) and Machine Learning. - Trained and evaluated ML models using TF-IDF, Logistic Regression, and Random Forest. - Built explainable prediction pipeline for transparency in classification. - Deployed interactive inference interface using Streamlit. Harry Potter Invisibility Cloak - Computer Vision App GitHub A real-time computer vision project that creates an invisibility effect using OpenCV and HSV color masking. - Implemented background subtraction and color segmentation for dynamic masking. - Integrated face detection using LBP Cascade Classifier for enhanced user interaction. - Created AR-style invisibility illusion with optimized frame processing. Data ETL Pipeline - Warehouse Analytics Platform GitHub A comprehensive data pipeline for automating warehouse shipment analytics using Google Cloud Platform (GCP) services. - Designed an ETL pipeline for CSV ingestion, cleaning, and transformation. - Integrated with BigQuery and Cloud Storage for real-time analytics. - Enabled business intelligence insights through automated data visualization dashboards. Education & Training Brototype - Data Science and AI/ML Program Aug. 2024 - Present Remote / Kochi, Kerala Comprehensive 1-year Data Science specialization program focused on Machine Learning, Deep Learning, MLOps, and Cloud Deployment. St. John's College, Anchal (University of Kerala) Aug. 2021 - Apr. 2024 Kollam, Kerala Bachelor of Commerce in Accounts and Data Science. CPHSS Kadakkal Jun. 2019 - May. 2021 Kollam, Kerala Higher Secondary Education in Commerce with Computer Applications. Key Achievements & Certifications - Developed and deployed multiple AI-powered applications leveraging FastAPI, LangChain, and LLMs for intelligent automation and RAG-based systems. - Implemented end-to-end ETL pipelines using Apache Airflow on AWS and GCP Cloud Run, BigQuery, Cloud SQL, and Cloud Storage. - Built and fine-tuned machine learning and deep learning models for real-world use cases in prediction, computer vision, and generative AI. - Deployed scalable microservice architectures on Google Cloud Kubernetes Engine (GKE) and achieved high reliability across services. - Integrated MLOps practices including CI/CD pipelines for model deployment and monitoring in production. - Hands-on expertise in Data Analytics, Python, SQL, and BI tools for actionable insights and data storytelling. - Earned certifications in Data Science, Machine Learning, and Cloud Computing through Brototype and self-driven projects.\n",
      "8893\n"
     ]
    }
   ],
   "source": [
    "# Clean\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize extracted CV text for LLM extraction.\"\"\"\n",
    "    \n",
    "    # Normalize Unicode and typographic symbols\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"\\(cid:\\d+\\)\", \"\", text)\n",
    "    text = re.sub(r\"√¢‚Ç¨‚Äú|√¢‚Ç¨‚Äù|‚Äì|‚Äî\", \"-\", text)\n",
    "    text = re.sub(r\"[‚Äú‚Äù]\", '\"', text)\n",
    "    text = re.sub(r\"[‚Äò‚Äô]\", \"'\", text)\n",
    "    text = re.sub(r\"‚Ä¢+\", \"‚Ä¢\", text)\n",
    "    text = re.sub(r\"---\\s*Page\\s*\\d+\\s*---\", \" \", text)\n",
    "\n",
    "    # Replace known icons/emojis with labels\n",
    "    replacements = {\n",
    "        \"üìç\": \"Location:\",\n",
    "        \"üìß\": \"Email:\",\n",
    "        \"üì±\": \"Phone:\",\n",
    "        \"üåê\": \"Website:\",\n",
    "        \"üíº\": \"LinkedIn:\",\n",
    "        \"üêô\": \"GitHub:\",\n",
    "        \"üè†\": \"Address:\",\n",
    "        \"‚òéÔ∏è\": \"Phone:\",\n",
    "        \"‚úâÔ∏è\": \"Email:\",\n",
    "    }\n",
    "    for k, v in replacements.items():\n",
    "        text = text.replace(k, v)\n",
    "\n",
    "    # Normalize bullets, newlines, and separators\n",
    "    text = re.sub(r'[\\u2022\\u25CF\\u25A0‚Ä¢‚ñ™]', '-', text)  # normalize bullets\n",
    "    text = re.sub(r'[-_]{3,}', ' ', text)\n",
    "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    # Strip emojis or pictographs (catch-all)\n",
    "    text = re.sub(r'[\\U00010000-\\U0010ffff]', ' ', text)  # remove all emojis\n",
    "\n",
    "    # Remove stray non-ASCII junk but keep letters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    # Normalize punctuation & spacing\n",
    "    text = re.sub(r'\\s([,.!?;:])', r'\\1', text)\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "# Example\n",
    "cleaned_text = clean_text(text)\n",
    "print(cleaned_text)\n",
    "print(len(cleaned_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901b342c-2455-434e-a3f7-4ae260b157b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning step for cleaning empty list[] or \"null\", to make it None \n",
    "def clean_empty_lists_as_none(data):\n",
    "    for key, value in data.items():\n",
    "        if value is None:\n",
    "            continue\n",
    "        if isinstance(value, str) and value.strip().lower() == \"null\":\n",
    "            data[key] = None\n",
    "        elif isinstance(value, str) and value.strip() == \"\":\n",
    "            data[key] = None\n",
    "        elif isinstance(value, list):\n",
    "            # Check if list is empty or contains only null/None\n",
    "            if len(value) == 0:\n",
    "                data[key] = None\n",
    "            elif all((v is None) or (isinstance(v, str) and v.strip().lower() == \"null\") for v in value):\n",
    "                data[key] = None\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a2ce0-d0d4-495d-bc96-c4062df610c4",
   "metadata": {},
   "source": [
    "## Testing RAG Chunk Approach with Chunk embedding also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6279aece-3ec6-491a-a8e8-44157aa3a259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\afzal\\Jupyter Workspace\\Week 52\\ETL project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks embedded\n",
      "Got index\n",
      "Prompt completed\n",
      "12008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'skills': ['Python',\n",
       "  'TensorFlow',\n",
       "  'PyTorch',\n",
       "  'scikit-learn',\n",
       "  'Pandas',\n",
       "  'NumPy',\n",
       "  'OpenCV',\n",
       "  'Transformers (LLMs)',\n",
       "  'NLP',\n",
       "  'Computer Vision',\n",
       "  'Generative AI (VAE, GAN)',\n",
       "  'Apache Spark',\n",
       "  'Hadoop',\n",
       "  'Kafka',\n",
       "  'AWS',\n",
       "  'GCP',\n",
       "  'Azure',\n",
       "  'SQL',\n",
       "  'PostgreSQL',\n",
       "  'MySQL',\n",
       "  'FastAPI',\n",
       "  'Microservices',\n",
       "  'Docker',\n",
       "  'Kubernetes',\n",
       "  'REST APIs',\n",
       "  'JWT Authentication',\n",
       "  'Matplotlib',\n",
       "  'Seaborn',\n",
       "  'Power BI',\n",
       "  'Tableau',\n",
       "  'System Design',\n",
       "  'Data Structures and Algorithms',\n",
       "  'Data Analytics'],\n",
       " 'education': ['Brototype - Data Science and AI/ML Program (Aug. 2024 - Present)',\n",
       "  \"St. John's College, Anchal (University of Kerala) (Aug. 2021 - Apr. 2024)\",\n",
       "  'CPHSS Kadakkal (Jun. 2019 - May. 2021)'],\n",
       " 'experience': None,\n",
       " 'projects': [{'name': 'CVAlyze - AI-Powered ETL + CV Analysis Platform',\n",
       "   'links': None},\n",
       "  {'name': 'True Buddy Chatbot - Emotional Support AI',\n",
       "   'links': ['https://article-researcher-app-cbngx5mmmxxdtjzfewngtn.streamlit.app/',\n",
       "    'https://github.com/me-Afzal/',\n",
       "    'https://github.com/me-Afzal/Article-Researcher-app',\n",
       "    'https://github.com/me-Afzal/Bank_Analysis',\n",
       "    'https://github.com/me-Afzal/Data-ETL-Pipeline',\n",
       "    'https://github.com/me-Afzal/Fake-News-Detection',\n",
       "    'https://github.com/me-Afzal/Harry_Potter_Cloak_Invisibility',\n",
       "    'https://github.com/me-Afzal/Hybrid-movie-recommendation-app',\n",
       "    'https://github.com/me-Afzal/IPL_win_probability_p']},\n",
       "  {'name': 'Article Research Tool (RAG App)',\n",
       "   'links': ['https://github.com/me-Afzal/',\n",
       "    'https://github.com/me-Afzal/Article-Researcher-app']}],\n",
       " 'certifications': None,\n",
       " 'achievements': ['Deployed AI applications leveraging FastAPI, LangChain, and LLMs',\n",
       "  'Implemented ETL pipelines using Apache Airflow on AWS and GCP',\n",
       "  'Built ML/DL models for prediction, CV, and generative AI',\n",
       "  'Deployed scalable microservices on Google Cloud Kubernetes Engine (GKE)',\n",
       "  'Integrated MLOps practices including CI/CD pipelines',\n",
       "  'Hands-on expertise in Data Analytics, Python, SQL, and BI tools',\n",
       "  'Earned certifications in Data Science, ML, and Cloud Computing']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, json, requests, faiss, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from my_secrets import API_KEY1\n",
    "\n",
    "def extract_rag_info(text, api_key):\n",
    "    \"\"\"\n",
    "    RAG-based CV info extractor using Gemini + FAISS retrieval.\n",
    "    \"\"\"\n",
    "    # Define extraction targets\n",
    "    queries = {\n",
    "        \"skills\": \"List all technical and professional skills mentioned in the resume.\",\n",
    "        \"education\": \"List all education qualifications or academic programs.\",\n",
    "        \"experience\": \"List all work roles or internships with organization names.\",\n",
    "        \"projects\": \"List all projects or major works mentioned.\",\n",
    "        \"certifications\": \"List all certifications or courses explicitly mentioned.\",\n",
    "        \"achievements\": \"List all major achievements or accomplishments.\"\n",
    "    }\n",
    "\n",
    "    # Split text into chunks\n",
    "    chunk_size = 600\n",
    "    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "    # Embed chunks\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    chunk_embeddings = model.encode(chunks, convert_to_numpy=True)\n",
    "    print(\"Chunks embedded\")\n",
    "    # Build FAISS index\n",
    "    dim = chunk_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(chunk_embeddings)\n",
    "    print(\"Got index\")\n",
    "    # Retrieve relevant chunks per query\n",
    "    retrieved = {}\n",
    "    for key, query in queries.items():\n",
    "        q_emb = model.encode([query], convert_to_numpy=True)\n",
    "        top_k = 8 if key == \"projects\" else 2\n",
    "        _, indices = index.search(q_emb, top_k)\n",
    "        top_chunks = [chunks[i] for i in indices[0]]\n",
    "        retrieved[key] = \"\\n\".join(top_chunks)\n",
    "\n",
    "    # Prepare the prompt\n",
    "    combined_prompt = \"\"\"\n",
    "You are a fast, precise resume information extraction system. Your **ONLY** output must be a single JSON object.\n",
    "\n",
    "**Required JSON Keys:** [skills, education, experience, projects, certifications, achievements]\n",
    "\n",
    "**Extraction Rules (Minimal Factual Lists - STRICT JSON NULL/ARRAY):**\n",
    "1. **skills:** List of skill names only (no sentences). Example: [\"Python\", \"SQL\", \"FastAPI\"].\n",
    "2. **education, experience, certifications, achievements:** Each as a **list of short single-line items.**\n",
    "   * education: Format \"Degree/Program ‚Äì Institution (Year Range)\".\n",
    "   * experience: Format \"Role ‚Äì Organization\".\n",
    "   * certifications: Format \"Certification Name or Issuer\".\n",
    "   * achievements: Short factual result, under 10 words.\n",
    "3. **projects:** List of objects `{\"name\": \"...\", \"links\": [...]}`.\n",
    "   * Include only GitHub, demo, or portfolio links relevant to the project.\n",
    "   * If no links, set \"links\": null.\n",
    "\n",
    "**Output Format Rules:**\n",
    "- STRICT JSON only.\n",
    "- Missing list fields ‚Üí `[]`\n",
    "- Missing single-value fields ‚Üí `null`\n",
    "- Keep all values concise and factual.\n",
    "\"\"\"\n",
    "\n",
    "    # Attach retrieved context\n",
    "    for key, context in retrieved.items():\n",
    "        combined_prompt += f\"\\n\\n### {key.upper()} SECTION CONTEXT ###\\n{context}\"\n",
    "    print('Prompt completed')\n",
    "    print(len(combined_prompt))\n",
    "    # Call Gemini API\n",
    "    api_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"X-goog-api-key\": api_key}\n",
    "    payload = {\"contents\": [{\"parts\": [{\"text\": combined_prompt}]}]}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload).json()\n",
    "        raw_text = response[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        clean_json_text = re.sub(r\"^```[a-zA-Z]*|```$\", \"\", raw_text).strip()\n",
    "        data = json.loads(clean_json_text)\n",
    "\n",
    "        # Clean nulls and empty lists\n",
    "        data = clean_empty_lists_as_none(data)\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting sections: {e}\")\n",
    "        return {key: None for key in queries.keys()}\n",
    "\n",
    "# ---------- Example Usage ----------\n",
    "API_KEY = API_KEY1\n",
    "info_dict = extract_rag_info(cleaned_text, API_KEY)\n",
    "info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8dbb9-c0c6-4308-b136-52af164cd900",
   "metadata": {},
   "source": [
    "### When we use RAG Top k Chunks method, it take 10 seconds to completely extract skills, education,experience,projects(not completely),certifications,acheievements, For this, I have used all-mpnet-base-v2,multi-qa-MiniLM-L6-cos-v1,all-MiniLM-L6-v2. -- Performance is better than using locally installed LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c64df2-c730-4ba0-95d9-9c28604940e8",
   "metadata": {},
   "source": [
    "# Testing - RAG without making chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38779859-9a72-48e5-ac16-03dbf5e147bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'AFZAL A',\n",
       " 'profession': 'Data Scientist',\n",
       " 'phone_number': '+91 7356922047',\n",
       " 'email': 'afzalkottukkal23@gmail.com',\n",
       " 'location': None,\n",
       " 'github_link': 'https://github.com/me-Afzal/',\n",
       " 'linkedin_link': 'https://linkedin.com/in/afzal-a-0b1962325',\n",
       " 'skills': ['Python',\n",
       "  'TensorFlow',\n",
       "  'PyTorch',\n",
       "  'scikit-learn',\n",
       "  'Pandas',\n",
       "  'NumPy',\n",
       "  'OpenCV',\n",
       "  'Transformers (LLMs)',\n",
       "  'NLP',\n",
       "  'Computer Vision',\n",
       "  'Generative AI (VAE, GAN)',\n",
       "  'Apache Spark',\n",
       "  'Hadoop',\n",
       "  'Kafka',\n",
       "  'AWS',\n",
       "  'GCP',\n",
       "  'Azure',\n",
       "  'SQL',\n",
       "  'PostgreSQL',\n",
       "  'MySQL',\n",
       "  'Query Optimization',\n",
       "  'Database Design',\n",
       "  'FastAPI',\n",
       "  'Microservices',\n",
       "  'Docker',\n",
       "  'Kubernetes',\n",
       "  'REST APIs',\n",
       "  'JWT Authentication',\n",
       "  'Matplotlib',\n",
       "  'Seaborn',\n",
       "  'Power BI',\n",
       "  'Tableau',\n",
       "  'Data Storytelling',\n",
       "  'System Design',\n",
       "  'Data Structures and Algorithms',\n",
       "  'RAG (Retrieval-Augmented Generation)',\n",
       "  'LangChain',\n",
       "  'Gemini API',\n",
       "  'ESRGAN',\n",
       "  'Streamlit',\n",
       "  'MLOps',\n",
       "  'CI/CD pipelines'],\n",
       " 'education': ['Data Science and AI/ML Program ‚Äì Brototype (Aug. 2024 - Present)',\n",
       "  \"Bachelor of Commerce in Accounts and Data Science ‚Äì St. John's College, Anchal (University of Kerala) (Aug. 2021 - Apr. 2024)\",\n",
       "  'Higher Secondary Education in Commerce with Computer Applications ‚Äì CPHSS Kadakkal (Jun. 2019 - May. 2021)'],\n",
       " 'experience': None,\n",
       " 'projects': [{'name': 'CVAlyze - AI-Powered ETL + CV Analysis Platform',\n",
       "   'links': []},\n",
       "  {'name': 'Image Enhancer Pro - ESRGAN Image Super-Resolution API',\n",
       "   'links': ['https://github.com/me-Afzal/Image-Enhancer-Pro']},\n",
       "  {'name': 'Bank Analytics Dashboard',\n",
       "   'links': ['https://github.com/me-Afzal/Bank_Analysis']},\n",
       "  {'name': 'Article Research Tool (RAG App)',\n",
       "   'links': ['https://article-researcher-app-cbngx5mmmxxdtjzfewngtn.streamlit.app/',\n",
       "    'https://github.com/me-Afzal/Article-Researcher-app']},\n",
       "  {'name': 'True Buddy Chatbot - Emotional Support AI',\n",
       "   'links': ['https://true-buddy-chatbot.streamlit.app/',\n",
       "    'https://github.com/me-Afzal/True-Buddy-Chatbot']},\n",
       "  {'name': 'Medical Insurance Cost Predictor',\n",
       "   'links': ['https://medical-cost-predictor-web.streamlit.app/',\n",
       "    'https://github.com/me-Afzal/Medical-cost-predictor']},\n",
       "  {'name': 'NoteKeeper - FastAPI Microservices Application',\n",
       "   'links': ['https://github.com/me-Afzal/NoteKeeper']},\n",
       "  {'name': 'IPL Win Probability Predictor',\n",
       "   'links': ['https://ipl-win-probability-predictor-tool.streamlit.app/',\n",
       "    'https://github.com/me-Afzal/IPL_win_probability_predictor']},\n",
       "  {'name': 'Movie Recommender Pro (Hybrid)',\n",
       "   'links': ['https://hybrid-movie-recommend-app.streamlit.app/',\n",
       "    'https://github.com/me-Afzal/Hybrid-movie-recommendation-app']},\n",
       "  {'name': 'Fake News Detector',\n",
       "   'links': ['https://github.com/me-Afzal/Fake-News-Detection']},\n",
       "  {'name': 'Harry Potter Invisibility Cloak - Computer Vision App',\n",
       "   'links': ['https://github.com/me-Afzal/Harry_Potter_Cloak_Invisibility']},\n",
       "  {'name': 'Data ETL Pipeline - Warehouse Analytics Platform',\n",
       "   'links': ['https://github.com/me-Afzal/Data-ETL-Pipeline']}],\n",
       " 'certifications': ['Data Science', 'Machine Learning', 'Cloud Computing'],\n",
       " 'achievements': ['Developed and deployed multiple AI-powered applications.',\n",
       "  'Implemented end-to-end ETL pipelines on AWS and GCP.',\n",
       "  'Built and fine-tuned ML and DL models for real-world use cases.',\n",
       "  'Deployed scalable microservice architectures on GKE.',\n",
       "  'Integrated MLOps practices including CI/CD pipelines.',\n",
       "  'Hands-on expertise in Data Analytics, Python, SQL, and BI tools.',\n",
       "  'Earned certifications in Data Science, ML, and Cloud Computing.']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract Resume Sections\n",
    "from my_secrets import API_KEY2\n",
    "\n",
    "def extract_info(text,api_key):\n",
    "\n",
    "    combined_prompt = \"\"\"\n",
    "You are a fast, precise resume information extraction system. Your **ONLY** output must be a single JSON object.\n",
    "\n",
    "**Required JSON Keys:** [name, profession, phone_number, email, location, github_link, linkedin_link, skills, education, experience, projects, certifications, achievements]\n",
    "\n",
    "**Extraction Rules (Minimal Factual Lists - STRICT JSON NULL/ARRAY):**\n",
    "\n",
    "1.  **Personal Info:** Return concise string value or **null** if missing. **location** must be the personal contact address **ONLY**. **IGNORE** all job/education/project locations.\n",
    "    * ***github\\_link and linkedin\\_link:*** Search the entire text for URLs containing 'github.com' or 'linkedin.com/in/' and extract them. If multiple are found, use the first one. Use **null** if not found.\n",
    "2.  **skills:** Extract all technical/professional skills as a **list of strings**. Use **[]** (empty list) if none found.\n",
    "3.  **education, experience, certifications, achievements:** Return each as a **list of single-line strings**. **NO SUMMARIES/EXPLANATIONS.**\n",
    "    * If a section is missing, return **[]** (empty list).\n",
    "    * **education:** Format: [Degree/Program ‚Äì Institution (Year Range)]. Include all programs/levels.\n",
    "    * **experience:** Format: [Role ‚Äì Organization].\n",
    "    * **certifications:** Format: [Certification Title or Issuer]. ***Only include certifications EXPLICITLY listed as such.***\n",
    "    * **achievements:** Format: **[Key Result, max 10 words]**. Summarize core action/outcome.\n",
    "4.  **projects:** Return a **list of objects**. Use **[]** (empty list) if no projects.\n",
    "    * Each object **must** adhere to: `{\"name\": \"...\", \"links\": [...]}`.\n",
    "    * **The `links` list MUST only contain project-specific URLs (GitHub/demo).** **Exclude all other links.** If no project-specific link is found, the `links` key must be **null**.\n",
    "\n",
    "**Final Mandate (For Maximum Speed and Precision):**\n",
    "* Output is **STRICTLY JSON** (no preamble/postscript).\n",
    "* **Missing list fields MUST use `[]`. Missing single-value fields MUST use `null`.**\n",
    "* Keep all values **EXTREMELY CONCISE AND MINIMAL.**\n",
    "\n",
    "**Text sections:**\n",
    "\"\"\"\n",
    "    # Personal info extraction text\n",
    "    combined_prompt+=text\n",
    "    print(len(combined_prompt))\n",
    "\n",
    "    # ---------- Call Gemini API once ----------\n",
    "    api_url = \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent\"\n",
    "    headers = {\"Content-Type\": \"application/json\", \"X-goog-api-key\": api_key}\n",
    "    payload = {\"contents\": [{\"parts\": [{\"text\": combined_prompt}]}]}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(api_url, headers=headers, json=payload).json()\n",
    "        raw_text = response[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
    "        clean_json_text = re.sub(r\"^```[a-zA-Z]*|```$\", \"\", raw_text).strip()\n",
    "        data = json.loads(clean_json_text)\n",
    "        data = clean_empty_lists_as_none(data)\n",
    "\n",
    "        return data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting sections: {e}\")\n",
    "        return response\n",
    "\n",
    "# ---------- Example Usage ----------\n",
    "API_KEY = API_KEY2\n",
    "info_dict = extract_info(cleaned_text, API_KEY)\n",
    "info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6f594e-776e-42ce-8ca8-296d06939c8c",
   "metadata": {},
   "source": [
    "### It give response much faster than chunks setup and local LLm setup. It gave response within 4 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f3bb7-8966-41e7-bacf-1d4327c78146",
   "metadata": {},
   "source": [
    "# We created custom class of this final RAG setup for cv extraction and created a preprocess.py for storing data preprocessing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e282fe3-1300-4e0e-97b2-1be8064a8da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>profession</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>email</th>\n",
       "      <th>location</th>\n",
       "      <th>github_link</th>\n",
       "      <th>linkedin_link</th>\n",
       "      <th>skills</th>\n",
       "      <th>education</th>\n",
       "      <th>experience</th>\n",
       "      <th>projects</th>\n",
       "      <th>certifications</th>\n",
       "      <th>achievements</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFZAL A</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+91 7356922047</td>\n",
       "      <td>afzalkottukkal23@gmail.com</td>\n",
       "      <td>None</td>\n",
       "      <td>https://github.com/me-Afzal/</td>\n",
       "      <td>https://linkedin.com/in/afzal-a-0b1962325</td>\n",
       "      <td>[Python, TensorFlow, PyTorch, scikit-learn, Pa...</td>\n",
       "      <td>[Brototype - Data Science and AI/ML Program (A...</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'name': 'CVAlyze - AI-Powered ETL + CV Analy...</td>\n",
       "      <td>None</td>\n",
       "      <td>[Deployed AI-powered apps using FastAPI, LangC...</td>\n",
       "      <td>20.5937</td>\n",
       "      <td>78.9629</td>\n",
       "      <td>India</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name      profession    phone_number                       email  \\\n",
       "0  AFZAL A  Data Scientist  +91 7356922047  afzalkottukkal23@gmail.com   \n",
       "\n",
       "  location                   github_link  \\\n",
       "0     None  https://github.com/me-Afzal/   \n",
       "\n",
       "                               linkedin_link  \\\n",
       "0  https://linkedin.com/in/afzal-a-0b1962325   \n",
       "\n",
       "                                              skills  \\\n",
       "0  [Python, TensorFlow, PyTorch, scikit-learn, Pa...   \n",
       "\n",
       "                                           education experience  \\\n",
       "0  [Brototype - Data Science and AI/ML Program (A...       None   \n",
       "\n",
       "                                            projects certifications  \\\n",
       "0  [{'name': 'CVAlyze - AI-Powered ETL + CV Analy...           None   \n",
       "\n",
       "                                        achievements  latitude  longitude  \\\n",
       "0  [Deployed AI-powered apps using FastAPI, LangC...   20.5937    78.9629   \n",
       "\n",
       "  country gender  \n",
       "0   India   male  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from preprocess import extract_text,clean_text,get_gender,get_lat_lon\n",
    "from rag_extractor import CvExtractor\n",
    "import pandas as pd\n",
    "source=\"og_cv/afzal.pdf\"\n",
    "cleaned_text=clean_text(extract_text(source))\n",
    "extractor=CvExtractor()\n",
    "info_dict=extractor.extract(cleaned_text)\n",
    "\n",
    "df=pd.DataFrame([info_dict])\n",
    "df[['latitude', 'longitude', 'country']]=df['location'].apply(lambda loc: pd.Series(get_lat_lon(loc)))\n",
    "df['gender']=df['name'].apply(get_gender)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855ba17e-493d-425c-b1e4-ffb0ef8cb1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>profession</th>\n",
       "      <th>phone_number</th>\n",
       "      <th>email</th>\n",
       "      <th>location</th>\n",
       "      <th>github_link</th>\n",
       "      <th>linkedin_link</th>\n",
       "      <th>skills</th>\n",
       "      <th>education</th>\n",
       "      <th>experience</th>\n",
       "      <th>projects</th>\n",
       "      <th>certifications</th>\n",
       "      <th>achievements</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afzal A</td>\n",
       "      <td>Data Scientist &amp; Machine Learning Engineer</td>\n",
       "      <td>+91-9876543210</td>\n",
       "      <td>afzal@email.com</td>\n",
       "      <td>Bangalore, India</td>\n",
       "      <td>https://github.com/afzala</td>\n",
       "      <td>https://linkedin.com/in/afzala</td>\n",
       "      <td>[Python, R, SQL, Spark, TensorFlow, PyTorch, s...</td>\n",
       "      <td>[M.Tech, Data Science ‚Äì Indian Institute of Te...</td>\n",
       "      <td>[Data Scientist ‚Äì Walmart Labs, Bangalore, Mac...</td>\n",
       "      <td>[{'name': 'Retail Analytics Dashboard', 'links...</td>\n",
       "      <td>[AWS Certified Machine Learning - Specialty, G...</td>\n",
       "      <td>[Won Top 10 AI Innovation Award at Walmart Hac...</td>\n",
       "      <td>12.976794</td>\n",
       "      <td>77.590082</td>\n",
       "      <td>India</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>John Smith</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+1 123-456-7890</td>\n",
       "      <td>john.smith@example.com</td>\n",
       "      <td>New York, USA</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Machine Learning (Scikit-learn, PyTorch, Tens...</td>\n",
       "      <td>[M.Sc. in Data Science ‚Äì New York University (...</td>\n",
       "      <td>[Data Scientist ‚Äì Walmart, Data Analyst ‚Äì Target]</td>\n",
       "      <td>None</td>\n",
       "      <td>[AWS Certified Machine Learning Specialist, Go...</td>\n",
       "      <td>[Developed demand forecasting models that impr...</td>\n",
       "      <td>40.712728</td>\n",
       "      <td>-74.006015</td>\n",
       "      <td>United States</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Doe</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+91-9000000000</td>\n",
       "      <td>john@email.com</td>\n",
       "      <td>New York, USA</td>\n",
       "      <td>https://github.com/john</td>\n",
       "      <td>https://linkedin.com/in/john</td>\n",
       "      <td>[R, Tableau, Power BI, Statistics, NLP, Time S...</td>\n",
       "      <td>[B.Sc, Computer Science ‚Äì New York University ...</td>\n",
       "      <td>[Data Scientist ‚Äì Company ABC]</td>\n",
       "      <td>[{'name': 'Customer Segmentation', 'links': No...</td>\n",
       "      <td>[Google Cloud Professional Data Engineer, Adva...</td>\n",
       "      <td>[Presented at Data Science Summit 2022, Publis...</td>\n",
       "      <td>40.712728</td>\n",
       "      <td>-74.006015</td>\n",
       "      <td>United States</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ivy Walker</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+91-9000000009</td>\n",
       "      <td>ivy@email.com</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>https://github.com/ivy</td>\n",
       "      <td>https://linkedin.com/in/ivy</td>\n",
       "      <td>[R, Tableau, Power BI, Statistics, NLP, Time S...</td>\n",
       "      <td>[B.Sc, Computer Science ‚Äì New York University ...</td>\n",
       "      <td>[Data Scientist ‚Äì Company ABC]</td>\n",
       "      <td>[{'name': 'Customer Segmentation', 'links': No...</td>\n",
       "      <td>[Google Cloud Professional Data Engineer, Adva...</td>\n",
       "      <td>[Published research paper on AI-driven Pricing...</td>\n",
       "      <td>1.357107</td>\n",
       "      <td>103.819499</td>\n",
       "      <td>Singapore</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jane Smith</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+91-9000000001</td>\n",
       "      <td>jane@email.com</td>\n",
       "      <td>Bangalore, India</td>\n",
       "      <td>https://github.com/jane</td>\n",
       "      <td>https://linkedin.com/in/jane</td>\n",
       "      <td>[AWS, GCP, Azure, Python, SQL, Machine Learnin...</td>\n",
       "      <td>[MBA, Business Analytics ‚Äì London Business Sch...</td>\n",
       "      <td>[Data Scientist ‚Äì Company ABC]</td>\n",
       "      <td>[{'name': 'Retail Analytics Dashboard', 'links...</td>\n",
       "      <td>[Google Cloud Professional Data Engineer, Adva...</td>\n",
       "      <td>[Developed award-winning ML model for customer...</td>\n",
       "      <td>12.976794</td>\n",
       "      <td>77.590082</td>\n",
       "      <td>India</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alice Johnson</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+91-9000000002</td>\n",
       "      <td>alice@email.com</td>\n",
       "      <td>Delhi, India</td>\n",
       "      <td>https://github.com/alice</td>\n",
       "      <td>https://linkedin.com/in/alice</td>\n",
       "      <td>[AWS, GCP, Azure, NLP, Time Series Forecasting...</td>\n",
       "      <td>[M.Sc, Statistics ‚Äì University of California (...</td>\n",
       "      <td>[Data Scientist ‚Äì Company ABC]</td>\n",
       "      <td>[{'name': 'Fraud Detection System', 'links': N...</td>\n",
       "      <td>[Advanced SQL for Data Science - Coursera, Goo...</td>\n",
       "      <td>[Won Top 10 AI Innovation Award at Walmart Hac...</td>\n",
       "      <td>28.632803</td>\n",
       "      <td>77.219771</td>\n",
       "      <td>India</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bob Brown</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+91-9000000003</td>\n",
       "      <td>bob@email.com</td>\n",
       "      <td>San Francisco, USA</td>\n",
       "      <td>https://github.com/bob</td>\n",
       "      <td>https://linkedin.com/in/bob</td>\n",
       "      <td>[AWS, GCP, Azure, Deep Learning, TensorFlow, P...</td>\n",
       "      <td>[MBA, Business Analytics ‚Äì London Business Sch...</td>\n",
       "      <td>[Data Scientist ‚Äì Company ABC]</td>\n",
       "      <td>[{'name': 'Retail Analytics Dashboard', 'links...</td>\n",
       "      <td>[Tableau Desktop Specialist, Google Cloud Prof...</td>\n",
       "      <td>[Published research paper on AI-driven Pricing...</td>\n",
       "      <td>37.779259</td>\n",
       "      <td>-122.419329</td>\n",
       "      <td>United States</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Charlie Davis</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+91-9000000004</td>\n",
       "      <td>charlie@email.com</td>\n",
       "      <td>Mumbai, India</td>\n",
       "      <td>https://github.com/charlie</td>\n",
       "      <td>https://linkedin.com/in/charlie</td>\n",
       "      <td>[Deep Learning, TensorFlow, PyTorch, Cloud Com...</td>\n",
       "      <td>[M.Sc, Statistics ‚Äì University of California (...</td>\n",
       "      <td>[Data Scientist ‚Äì Company ABC]</td>\n",
       "      <td>[{'name': 'E-commerce Chatbot', 'links': None}...</td>\n",
       "      <td>[AWS Certified Machine Learning - Specialty, T...</td>\n",
       "      <td>[Recognized as Employee of the Year 2021 at Co...</td>\n",
       "      <td>19.054999</td>\n",
       "      <td>72.869203</td>\n",
       "      <td>India</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Eva Wilson</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+91-9000000005</td>\n",
       "      <td>eva@email.com</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>https://github.com/eva</td>\n",
       "      <td>https://linkedin.com/in/eva</td>\n",
       "      <td>[Python, SQL, Machine Learning, Data Analysis,...</td>\n",
       "      <td>[B.Sc, Computer Science ‚Äì New York University ...</td>\n",
       "      <td>[Data Scientist ‚Äì Company ABC]</td>\n",
       "      <td>[{'name': 'Fraud Detection System', 'links': N...</td>\n",
       "      <td>[AWS Certified Machine Learning - Specialty, M...</td>\n",
       "      <td>[Developed award-winning ML model for customer...</td>\n",
       "      <td>51.489334</td>\n",
       "      <td>-0.144055</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Frank Miller</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>+91-9000000006</td>\n",
       "      <td>frank@email.com</td>\n",
       "      <td>Toronto, Canada</td>\n",
       "      <td>https://github.com/frank</td>\n",
       "      <td>https://linkedin.com/in/frank</td>\n",
       "      <td>[Python, SQL, Machine Learning, Data Analysis,...</td>\n",
       "      <td>[MBA, Business Analytics ‚Äì London Business Sch...</td>\n",
       "      <td>[Data Scientist ‚Äì Company ABC]</td>\n",
       "      <td>[{'name': 'Dynamic Pricing Engine', 'links': N...</td>\n",
       "      <td>[Microsoft Azure AI Fundamentals, Google Cloud...</td>\n",
       "      <td>[Presented at Data Science Summit 2022, Publis...</td>\n",
       "      <td>43.653482</td>\n",
       "      <td>-79.383935</td>\n",
       "      <td>Canada</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name                                  profession     phone_number  \\\n",
       "0        Afzal A  Data Scientist & Machine Learning Engineer   +91-9876543210   \n",
       "1     John Smith                              Data Scientist  +1 123-456-7890   \n",
       "2       John Doe                              Data Scientist   +91-9000000000   \n",
       "3     Ivy Walker                              Data Scientist   +91-9000000009   \n",
       "4     Jane Smith                              Data Scientist   +91-9000000001   \n",
       "5  Alice Johnson                              Data Scientist   +91-9000000002   \n",
       "6      Bob Brown                              Data Scientist   +91-9000000003   \n",
       "7  Charlie Davis                              Data Scientist   +91-9000000004   \n",
       "8     Eva Wilson                              Data Scientist   +91-9000000005   \n",
       "9   Frank Miller                              Data Scientist   +91-9000000006   \n",
       "\n",
       "                    email            location                 github_link  \\\n",
       "0         afzal@email.com    Bangalore, India   https://github.com/afzala   \n",
       "1  john.smith@example.com       New York, USA                        None   \n",
       "2          john@email.com       New York, USA     https://github.com/john   \n",
       "3           ivy@email.com           Singapore      https://github.com/ivy   \n",
       "4          jane@email.com    Bangalore, India     https://github.com/jane   \n",
       "5         alice@email.com        Delhi, India    https://github.com/alice   \n",
       "6           bob@email.com  San Francisco, USA      https://github.com/bob   \n",
       "7       charlie@email.com       Mumbai, India  https://github.com/charlie   \n",
       "8           eva@email.com          London, UK      https://github.com/eva   \n",
       "9         frank@email.com     Toronto, Canada    https://github.com/frank   \n",
       "\n",
       "                     linkedin_link  \\\n",
       "0   https://linkedin.com/in/afzala   \n",
       "1                             None   \n",
       "2     https://linkedin.com/in/john   \n",
       "3      https://linkedin.com/in/ivy   \n",
       "4     https://linkedin.com/in/jane   \n",
       "5    https://linkedin.com/in/alice   \n",
       "6      https://linkedin.com/in/bob   \n",
       "7  https://linkedin.com/in/charlie   \n",
       "8      https://linkedin.com/in/eva   \n",
       "9    https://linkedin.com/in/frank   \n",
       "\n",
       "                                              skills  \\\n",
       "0  [Python, R, SQL, Spark, TensorFlow, PyTorch, s...   \n",
       "1  [Machine Learning (Scikit-learn, PyTorch, Tens...   \n",
       "2  [R, Tableau, Power BI, Statistics, NLP, Time S...   \n",
       "3  [R, Tableau, Power BI, Statistics, NLP, Time S...   \n",
       "4  [AWS, GCP, Azure, Python, SQL, Machine Learnin...   \n",
       "5  [AWS, GCP, Azure, NLP, Time Series Forecasting...   \n",
       "6  [AWS, GCP, Azure, Deep Learning, TensorFlow, P...   \n",
       "7  [Deep Learning, TensorFlow, PyTorch, Cloud Com...   \n",
       "8  [Python, SQL, Machine Learning, Data Analysis,...   \n",
       "9  [Python, SQL, Machine Learning, Data Analysis,...   \n",
       "\n",
       "                                           education  \\\n",
       "0  [M.Tech, Data Science ‚Äì Indian Institute of Te...   \n",
       "1  [M.Sc. in Data Science ‚Äì New York University (...   \n",
       "2  [B.Sc, Computer Science ‚Äì New York University ...   \n",
       "3  [B.Sc, Computer Science ‚Äì New York University ...   \n",
       "4  [MBA, Business Analytics ‚Äì London Business Sch...   \n",
       "5  [M.Sc, Statistics ‚Äì University of California (...   \n",
       "6  [MBA, Business Analytics ‚Äì London Business Sch...   \n",
       "7  [M.Sc, Statistics ‚Äì University of California (...   \n",
       "8  [B.Sc, Computer Science ‚Äì New York University ...   \n",
       "9  [MBA, Business Analytics ‚Äì London Business Sch...   \n",
       "\n",
       "                                          experience  \\\n",
       "0  [Data Scientist ‚Äì Walmart Labs, Bangalore, Mac...   \n",
       "1  [Data Scientist ‚Äì Walmart, Data Analyst ‚Äì Target]   \n",
       "2                     [Data Scientist ‚Äì Company ABC]   \n",
       "3                     [Data Scientist ‚Äì Company ABC]   \n",
       "4                     [Data Scientist ‚Äì Company ABC]   \n",
       "5                     [Data Scientist ‚Äì Company ABC]   \n",
       "6                     [Data Scientist ‚Äì Company ABC]   \n",
       "7                     [Data Scientist ‚Äì Company ABC]   \n",
       "8                     [Data Scientist ‚Äì Company ABC]   \n",
       "9                     [Data Scientist ‚Äì Company ABC]   \n",
       "\n",
       "                                            projects  \\\n",
       "0  [{'name': 'Retail Analytics Dashboard', 'links...   \n",
       "1                                               None   \n",
       "2  [{'name': 'Customer Segmentation', 'links': No...   \n",
       "3  [{'name': 'Customer Segmentation', 'links': No...   \n",
       "4  [{'name': 'Retail Analytics Dashboard', 'links...   \n",
       "5  [{'name': 'Fraud Detection System', 'links': N...   \n",
       "6  [{'name': 'Retail Analytics Dashboard', 'links...   \n",
       "7  [{'name': 'E-commerce Chatbot', 'links': None}...   \n",
       "8  [{'name': 'Fraud Detection System', 'links': N...   \n",
       "9  [{'name': 'Dynamic Pricing Engine', 'links': N...   \n",
       "\n",
       "                                      certifications  \\\n",
       "0  [AWS Certified Machine Learning - Specialty, G...   \n",
       "1  [AWS Certified Machine Learning Specialist, Go...   \n",
       "2  [Google Cloud Professional Data Engineer, Adva...   \n",
       "3  [Google Cloud Professional Data Engineer, Adva...   \n",
       "4  [Google Cloud Professional Data Engineer, Adva...   \n",
       "5  [Advanced SQL for Data Science - Coursera, Goo...   \n",
       "6  [Tableau Desktop Specialist, Google Cloud Prof...   \n",
       "7  [AWS Certified Machine Learning - Specialty, T...   \n",
       "8  [AWS Certified Machine Learning - Specialty, M...   \n",
       "9  [Microsoft Azure AI Fundamentals, Google Cloud...   \n",
       "\n",
       "                                        achievements   latitude   longitude  \\\n",
       "0  [Won Top 10 AI Innovation Award at Walmart Hac...  12.976794   77.590082   \n",
       "1  [Developed demand forecasting models that impr...  40.712728  -74.006015   \n",
       "2  [Presented at Data Science Summit 2022, Publis...  40.712728  -74.006015   \n",
       "3  [Published research paper on AI-driven Pricing...   1.357107  103.819499   \n",
       "4  [Developed award-winning ML model for customer...  12.976794   77.590082   \n",
       "5  [Won Top 10 AI Innovation Award at Walmart Hac...  28.632803   77.219771   \n",
       "6  [Published research paper on AI-driven Pricing...  37.779259 -122.419329   \n",
       "7  [Recognized as Employee of the Year 2021 at Co...  19.054999   72.869203   \n",
       "8  [Developed award-winning ML model for customer...  51.489334   -0.144055   \n",
       "9  [Presented at Data Science Summit 2022, Publis...  43.653482  -79.383935   \n",
       "\n",
       "          country  gender  \n",
       "0           India    male  \n",
       "1   United States    male  \n",
       "2   United States    male  \n",
       "3       Singapore  female  \n",
       "4           India  female  \n",
       "5           India  female  \n",
       "6   United States    male  \n",
       "7           India    male  \n",
       "8  United Kingdom  female  \n",
       "9          Canada    male  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from preprocess import extract_text, clean_text, get_gender, get_lat_lon\n",
    "from rag_extractor import CvExtractor\n",
    "\n",
    "# Folder containing CV PDFs\n",
    "cv_folder = \"cv\"\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = CvExtractor()\n",
    "\n",
    "# List to store each CV's info\n",
    "all_cv_data = []\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for file_name in os.listdir(cv_folder):\n",
    "    if not file_name.lower().endswith((\".pdf\", \".docx\", \".txt\")):\n",
    "        continue  # skip other file types\n",
    "    file_path = os.path.join(cv_folder, file_name)\n",
    "    try:\n",
    "        # Extract and clean text (extract_text handles pdf, docx, txt)\n",
    "        text = clean_text(extract_text(file_path))\n",
    "        \n",
    "        # Extract CV info\n",
    "        info_dict = extractor.extract(text)\n",
    "        \n",
    "        # Append to list\n",
    "        all_cv_data.append(info_dict)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "df=pd.DataFrame(all_cv_data)\n",
    "df=df[['name', 'profession', 'phone_number', 'email', 'location', 'github_link', 'linkedin_link', 'skills',\n",
    "       'education', 'experience', 'projects', 'certifications', 'achievements']]\n",
    "df[['latitude', 'longitude', 'country']]=df['location'].apply(lambda loc: pd.Series(get_lat_lon(loc)))\n",
    "df['gender']=df['name'].apply(get_gender)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6cbcaf-3477-4f88-b3ad-1bb59c42c75f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "etl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
